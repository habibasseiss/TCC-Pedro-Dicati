\begin{resumo}[Abstract]
	
	\begin{otherlanguage*}{english}
		
	Digital accessibility is a crucial challenge in an increasingly connected world, especially for people with visual impairments. This paper presents the development of a cross-platform assistive application capable of capturing environmental images and describing them in audio using open-source large language models (LLMs). The application integrates computer vision and text-to-speech (TTS) technologies, promoting greater autonomy for visually impaired users. The methodology included benchmarking three AI models (Qwen 2.5, llava v1.6 Mistral e Llama 3.2 Vision), evaluated based on latency metrics, textual quality (BERTScore and ROUGE-L), and practical tests in real-world scenarios. The results showed that the Qwen 2.5 model achieved the best balance between descriptive accuracy and real-time performance. The developed prototype demonstrates the potential of AI to enhance digital inclusion, standing out as an innovative solution to improve the quality of life for visually impaired individuals.
	
    \textbf{Keywords}: Digital Accessibility, Assistive Technology, Multimodal Language Models, Computer Vision, Social Inclusion.
		
	\end{otherlanguage*}

\end{resumo} 