@misc{WHO2023,
    title = {Blindness and vision impairment},
    author = {OMS},
    year = {2023},
    url = {https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment},
    urldate = {20/01/2025}
}

@article{silvaneto2024tecnologias,
  author = {Raimundo Cazuza da Silva Neto and others},
  title = {Tecnologias assistivas como ferramentas mediadoras no processo de ensino e aprendizagem de alunos com deficiência visual: uma revisão sistemática},
  journal = {Journal Of Humanities And Social Science},
  volume = {29},
  number = {1},
  pages = {45-50},
  month = {January},
  year = {2024},
  doi = {10.9790/0837-2901074550},
  url = {https://www.iosrjournals.org/iosr-jhss/papers/Vol.29-Issue1/Ser-7/F2901074550.pdf},
  urldate = {20/01/2025}  
}

@article{lecun2015deeplearning,
  author = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
  title = {Deep Learning},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436-444},
  year = {2015},
  month = {May},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1038/nature14539},
  url = {http://dx.doi.org/10.1038/nature14539}
}

% dataset
@misc{bromonschenkel2024cocopt,
  title        = {COCO Captions Dataset Translation for Portuguese Image Captioning},
  author       = {Bromonschenkel, Gabriel and Oliveira, Hil{\'a}rio and Paix{\~a}o, Thiago M.},
  howpublished = {\url{https://huggingface.co/datasets/laicsiifes/coco-captions-pt-br}},
  publisher    = {Hugging Face},
  year         = {2024}
}

@misc{Baevski2020,
  doi = {10.48550/ARXIV.2006.11477},
  url = {https://arxiv.org/abs/2006.11477},
  author = {Baevski,  Alexei and Zhou,  Henry and Mohamed,  Abdelrahman and Auli,  Michael},
  keywords = {Computation and Language (cs.CL),  Machine Learning (cs.LG),  Sound (cs.SD),  Audio and Speech Processing (eess.AS),  FOS: Computer and information sciences,  FOS: Computer and information sciences,  FOS: Electrical engineering,  electronic engineering,  information engineering,  FOS: Electrical engineering,  electronic engineering,  information engineering},
  title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@MASTERSTHESIS {bala2024,
    author = "Ankith Bala",
    title  = "Multimodal LLM using Federated Visual Instruction Tuning for Visually Impaired",
    school = "University at Buffalo",
    year   = "2024",
    month  = "may",
    url = "{https://cse.buffalo.edu/tech-reports/2024-08.pdf}"
}

@BOOK {Ballard1982,
    author    = "Dana H. Ballard and Christopher M. Brown",
    title     = "Computer Vision",
    publisher = "Prentice Hall",
    year      = "1982",
    volume    = "1",
    address   = "Englewood Cliffs, New Jersey 07632",
    edition   = "first",
    url  = "{https://homepages.inf.ed.ac.uk/rbf/BOOKS/BANDB/Ballard__D._and_Brown__C._M.__1982__Computer_Vision.pdf}"
}

@inproceedings{banerjee2005,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "{https://aclanthology.org/W05-0909/}",
    pages = "65--72"
}

@MISC {bem2024,
    author = "Rafael Almeida de Bem",
    title  = "Uso de IA Generativa no Apoio à Aprendizagem de Programação",
    month  = "jul",
    year   = "2024",
    note   = "TCC",
    url = "{https://repositorio.pucrs.br/dspace/bitstream/10923/26764/3/2024_1_RAFAEL_ALMEIDA_DE_BEM_TCC.pdf}"
}

@MISC {bersch2024,
    author       = "Rita Bersch",
    title        = "Introdução à Tecnologia Assistiva",
    howpublished = "Porto Alegre",
    year         = "2017",
    url          = "{https://www.assistiva.com.br/Introducao_Tecnologia_Assistiva.pdf}"
}

@inproceedings{AmmarBouhamed2012,
  title = {New electronic cane for visually impaired people for obstacle detection and recognition},
  url = {http://dx.doi.org/10.1109/ICVES.2012.6294266},
  DOI = {10.1109/icves.2012.6294266},
  booktitle = {2012 IEEE International Conference on Vehicular Electronics and Safety (ICVES 2012)},
  publisher = {IEEE},
  author = {Ammar Bouhamed,  Sonda and Frikha Eleuch,  Jihen and Khanfir Kallel,  Imen and Sellami Masmoudi,  Dorra},
  year = {2012},
  month = jul,
  pages = {416–420},
  keywords={Sensor phenomena and characterization;Acoustics;Cameras;Measurement by laser beam;Intelligent sensors;Laser feedback},
}

@article{Botelho2024,
  title = {Trajetória da identifica\c{c}ão das pessoas com defici\^encia no Brasil: uma análise das pesquisas domiciliares do IBGE},
  volume = {29},
  ISSN = {1413-8123},
  url = {http://dx.doi.org/10.1590/1413-812320242911.03932024},
  DOI = {10.1590/1413-812320242911.03932024},
  number = {11},
  journal = {Ci\^encia |& Saúde Coletiva},
  publisher = {FapUNIFESP (SciELO)},
  author = {Botelho,  Luanda Chaves and Lenzi,  Maíra Bonna},
  year = {2024}
}

@misc{brilli2024,
  doi = {10.48550/ARXIV.2405.07606},
  url = {https://arxiv.org/abs/2405.07606},
  author = {Brilli,  Dionysia Danai and Georgaras,  Evangelos and Tsilivaki,  Stefania and Melanitis,  Nikos and Nikita,  Konstantina},
  keywords = {Human-Computer Interaction (cs.HC),  Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {AIris: An AI-powered Wearable Assistive Device for the Visually Impaired},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Bruno2019,
  title = {Política de Acessibilidade: o que dizem as pessoas com defici\^encia visual},
  volume = {44},
  ISSN = {0100-3143},
  url = {http://dx.doi.org/10.1590/2175-623684848},
  DOI = {10.1590/2175-623684848},
  number = {1},
  journal = {Educa\c{c}ão \& Realidade},
  publisher = {FapUNIFESP (SciELO)},
  author = {Bruno,  Marilda Moraes Garcia and Nascimento,  Ricardo Augusto Lins do},
  year = {2019}
}

@misc{Castano2023,
  doi = {10.48550/ARXIV.2311.13380},
  url = {https://arxiv.org/abs/2311.13380},
  author = {Castaño,  Joel and Martínez-Fernández,  Silverio and Franch,  Xavier and Bogner,  Justus},
  keywords = {Software Engineering (cs.SE),  Artificial Intelligence (cs.AI),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Analyzing the Evolution and Maintenance of ML Models on Hugging Face},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Castro2008,
  title = {Defici\^encia visual,  auditiva e física: preval\^encia e fatores associados em estudo de base populacional},
  volume = {24},
  ISSN = {0102-311X},
  url = {http://dx.doi.org/10.1590/s0102-311x2008000800006},
  DOI = {10.1590/s0102-311x2008000800006},
  number = {8},
  journal = {Cadernos de Saúde Pública},
  publisher = {FapUNIFESP (SciELO)},
  author = {Castro,  Shamyr Sulyvan de and César,  Chester Luiz Galvão and Carandina,  Luana and Barros,  Marilisa Berti Azevedo and Alves,  Maria Cecília Goi Porto and Goldbaum,  Moises},
  year = {2008},
  month = aug,
  pages = {1773–1782}
}

@article{CHILINGUE2024, 
    place={São Carlos-SP}, 
    title={A EAD COMO FERRAMENTA INCLUSIVA E DE ACESSIBILIDADE PARA DEFICIENTES VISUAIS NO ÂMBITO DO INSTITUTO BENJAMIN CONSTANT}, 
    volume={5},  
    url={https://ciet.ufscar.br/submissao/index.php/ciet/article/view/1079}, 
    abstractNote={O presente trabalho é fruto de desdobramento de ações desenvolvidas no âmbito do Instituto Benjamin Constant – IBC, situado na Cidade do Rio de Janeiro, referência nacional na educação de deficientes visuais. As ações ocorreram nas turmas de informática educativa da divisão de reabilitação – DRT - para alunos jovens e adultos que objetivam a inserção das ferramentas de tecnologia da informação e comunicação (TIC) para inserção dos alunos com deficiência visual na Educação a Distância, seja para o ensino básico, técnico ou superior. Para tanto, faz-se necessário atender aos requisitos mínimos de acessibilidade dos ambientes virtuais de aprendizagem, das tecnologias assistivas computacionais, as páginas de internet. O estudo contempla as dificuldades encontradas por esse alunado, bem como as necessidades e soluções apresentadas para promover a acessibilidade e, consequentemente, a inclusão digital e social. Após a realização das aulas-pesquisas-estudos, tem-se a possibilidade de destacar e apresentar e sugerir alternativas para suprir essa demanda em sua totalidade e/ou parcialidade, permitindo, ao menos, diminuir o hiato existente entre os usuários não videntes e os deficientes visuais.}, 
    number={1}, 
    journal={Anais CIET:Horizonte}, 
    author={CHILINGUE, MARCELO BUSTAMANTE}, 
    year={2024}, 
    month={abr.} 
}

@article{Dognin2022,
  title = {Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge},
  volume = {73},
  ISSN = {1076-9757},
  url = {http://dx.doi.org/10.1613/jair.1.13113},
  DOI = {10.1613/jair.1.13113},
  journal = {Journal of Artificial Intelligence Research},
  publisher = {AI Access Foundation},
  author = {Dognin,  Pierre and Melnyk,  Igor and Mroueh,  Youssef and Padhi,  Inkit and Rigotti,  Mattia and Ross,  Jarret and Schiff,  Yair and Young,  Richard A. and Belgodere,  Brian},
  year = {2022},
  month = jan,
  pages = {437–459}
}

@misc{dosovitskiy2020,
  doi = {10.48550/ARXIV.2010.11929},
  url = {https://arxiv.org/abs/2010.11929},
  author = {Dosovitskiy,  Alexey and Beyer,  Lucas and Kolesnikov,  Alexander and Weissenborn,  Dirk and Zhai,  Xiaohua and Unterthiner,  Thomas and Dehghani,  Mostafa and Minderer,  Matthias and Heigold,  Georg and Gelly,  Sylvain and Uszkoreit,  Jakob and Houlsby,  Neil},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Artificial Intelligence (cs.AI),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@inproceedings{Dutoit1997,
  title={High-quality text-to-speech synthesis : an overview},
  booktitle = "Journal Of Electrical And Electronics Engineering Australia",
  author={Thierry Dutoit},
  year={1997},
  url={https://api.semanticscholar.org/CorpusID:10693488},
  pages = {25-36},
}

@BOOK {gonzalez2018,
    author    = "Rafael C. Gonzalez and Richard E. Woods",
    title     = "Digital Image Processing",
    publisher = "Pearson",
    year      = "2018",
    address   = "330 Hudson Street, New York, NY 10013",
    edition   = "fourth",
    url       ="{https://dl.icdst.org/pdfs/files4/01c56e081202b62bd7d3b4f8545775fb.pdf}"
}

@inproceedings{hessel-etal-2021-clipscore,
    title = "{CLIPS}core: A Reference-free Evaluation Metric for Image Captioning",
    author = "Hessel, Jack  and
      Holtzman, Ari  and
      Forbes, Maxwell  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.595/",
    doi = "10.18653/v1/2021.emnlp-main.595",
    pages = "7514--7528",
    abstract = "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge."
}

@inproceedings{Jogin2018,
  title = {Feature Extraction using Convolution Neural Networks (CNN) and Deep Learning},
  url = {http://dx.doi.org/10.1109/RTEICT42901.2018.9012507},
  DOI = {10.1109/rteict42901.2018.9012507},
  booktitle = {2018 3rd IEEE International Conference on Recent Trends in Electronics,  Information &amp; Communication Technology (RTEICT)},
  publisher = {IEEE},
  author = {Jogin,  Manjunath and Mohana and Madhulika,  M S and Divya,  G D and Meghana,  R K and Apoorva,  S},
  year = {2018},
  month = may,
  pages = {2319–2323}
}


@InProceedings{radford2021,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@misc{He2015,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He,  Kaiming and Zhang,  Xiangyu and Ren,  Shaoqing and Sun,  Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Hossain2019,
  title = {A Comprehensive Survey of Deep Learning for Image Captioning},
  volume = {51},
  ISSN = {1557-7341},
  url = {http://dx.doi.org/10.1145/3295748},
  DOI = {10.1145/3295748},
  number = {6},
  journal = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  author = {Hossain,  MD. Zakir and Sohel,  Ferdous and Shiratuddin,  Mohd Fairuz and Laga,  Hamid},
  year = {2019},
  month = feb,
  pages = {1–36}
}

@ONLINE {huggingface2023,
    author = "Hugging Face",
    title  = "Hugging Face Hub documentation",
    year   = "2023",
    url    = "https://huggingface.co/docs/hub/index"
}

@article{LeCun2015,
  title = {Deep learning},
  volume = {521},
  ISSN = {1476-4687},
  url = {http://dx.doi.org/10.1038/nature14539},
  DOI = {10.1038/nature14539},
  number = {7553},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {LeCun,  Yann and Bengio,  Yoshua and Hinton,  Geoffrey},
  year = {2015},
  month = may,
  pages = {436–444}
}

@misc{lin2024,
  doi = {10.48550/ARXIV.2404.01291},
  url = {https://arxiv.org/abs/2404.01291},
  author = {Lin,  Zhiqiu and Pathak,  Deepak and Li,  Baiqi and Li,  Jiayao and Xia,  Xide and Neubig,  Graham and Zhang,  Pengchuan and Ramanan,  Deva},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Artificial Intelligence (cs.AI),  Computation and Language (cs.CL),  Machine Learning (cs.LG),  Multimedia (cs.MM),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Evaluating Text-to-Visual Generation with Image-to-Text Generation},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@inproceedings{NEURIPS2023_6dcf277e,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34892--34916},
 publisher = {Curran Associates, Inc.},
 title = {Visual Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{Papineni2001,
  series = {ACL ’02},
  title = {BLEU: a method for automatic evaluation of machine translation},
  url = {http://dx.doi.org/10.3115/1073083.1073135},
  DOI = {10.3115/1073083.1073135},
  booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics  - ACL ’02},
  publisher = {Association for Computational Linguistics},
  author = {Papineni,  Kishore and Roukos,  Salim and Ward,  Todd and Zhu,  Wei-Jing},
  year = {2001},
  pages = {311},
  collection = {ACL ’02}
}

@BOOK{Phillips1994,
  title     = "Image Processing in C",
  author    = "Phillips, Dwayne",
  publisher = "Prentice Hall PTR",
  month     =  jan,
  year      =  2000,
  address   = "Harlow, England"
}


@InProceedings{popov2021,
  title = 	 {Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech},
  author =       {Popov, Vadim and Vovk, Ivan and Gogoryan, Vladimir and Sadekova, Tasnima and Kudinov, Mikhail},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8599--8608},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/popov21a/popov21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/popov21a.html},
  abstract = 	 {Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score.}
}

@inproceedings{radford2023,
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
title = {Robust speech recognition via large-scale weak supervision},
year = {2023},
publisher = {JMLR.org},
abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1182},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{Sonth2017,
  title = {OCR based facilitator for the visually challenged},
  url = {http://dx.doi.org/10.1109/ICEECCOT.2017.8284628},
  DOI = {10.1109/iceeccot.2017.8284628},
  booktitle = {2017 International Conference on Electrical,  Electronics,  Communication,  Computer,  and Optimization Techniques (ICEECCOT)},
  publisher = {IEEE},
  author = {Sonth,  Shalini and Kallimani,  Jagadish S.},
  year = {2017},
  month = dec,
  pages = {1–7}
}

@misc{romeo2019,
  doi = {10.48550/ARXIV.1911.06727},
  url = {https://arxiv.org/abs/1911.06727},
  author = {Romeo,  Katerine and Pissaloux,  Edwige and Serin,  Frédéric},
  keywords = {Human-Computer Interaction (cs.HC),  Multimedia (cs.MM),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Accessibility to textual and visual information on websites for visually impaired persons},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}


@InProceedings{tan2019,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/tan19a.html},
  abstract = 	 {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}

@BOOK {szeliski2021,
    author    = "Richard Szeliski",
    title     = "Computer Vision: Algorithms and Applications",
    publisher = "Springer",
    year      = "2021",
    edition   = "second",
    month     = "sep",
    url       = "{https://szeliski.org/Book/}"
}

@article{Torres2002,
  title = {A acessibilidade à informa\c{c}ão no espa\c{c}o digital},
  volume = {31},
  ISSN = {0100-1965},
  url = {http://dx.doi.org/10.1590/s0100-19652002000300009},
  DOI = {10.1590/s0100-19652002000300009},
  number = {3},
  journal = {Ci\^encia da Informa\c{c}ão},
  publisher = {IBICT},
  author = {Torres,  Elisabeth Fátima and Mazzoni,  Alberto Angel and Alves,  João Bosco da Mota},
  year = {2002},
  month = sep,
  pages = {83–91}
}

@ARTICLE {trivedi2018,
    author  = "Ayushi Trivedi and others",
    title   = "Speech to text and text to speech recognition systems-Areview",
    journal = "IOSR Journal of Computer Engineering (IOSR-JCE)",
    year    = "2018",
    volume  = "20",
    number  = "2",
    pages   = "36-43",
    month   = "apr",
    url     = "{https://www.iosrjournals.org/iosr-jce/papers/Vol20-issue2/Version-1/E2002013643.pdf}"
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Vedantam2015,
  title = {CIDEr: Consensus-based image description evaluation},
  url = {http://dx.doi.org/10.1109/CVPR.2015.7299087},
  DOI = {10.1109/cvpr.2015.7299087},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author = {Vedantam,  Ramakrishna and Zitnick,  C. Lawrence and Parikh,  Devi},
  year = {2015},
  month = jun,
  pages = {4566–4575}
}

@InProceedings{wang2010,
author="Wang, Kai
and Belongie, Serge",
editor="Daniilidis, Kostas
and Maragos, Petros
and Paragios, Nikos",
title="Word Spotting in the Wild",
booktitle="Computer Vision -- ECCV 2010",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="591--604",
abstract="We present a method for spotting words in the wild, i.e., in real images taken in unconstrained environments. Text found in the wild has a surprising range of difficulty. At one end of the spectrum, Optical Character Recognition (OCR) applied to scanned pages of well formatted printed text is one of the most successful applications of computer vision to date. At the other extreme lie visual CAPTCHAs -- text that is constructed explicitly to fool computer vision algorithms. Both tasks involve recognizing text, yet one is nearly solved while the other remains extremely challenging. In this work, we argue that the appearance of words in the wild spans this range of difficulties and propose a new word recognition approach based on state-of-the-art methods from generic object recognition, in which we consider object categories to be the words themselves. We compare performance of leading OCR engines -- one open source and one proprietary -- with our new approach on the ICDAR Robust Reading data set and a new word spotting data set we introduce in this paper: the Street View Text data set. We show improvements of up to 16{\%} on the data sets, demonstrating the feasibility of a new approach to a seemingly old problem.",
isbn="978-3-642-15549-9"
}

@misc{wu2023,
  doi = {10.48550/ARXIV.2309.05519},
  url = {https://arxiv.org/abs/2309.05519},
  author = {Wu,  Shengqiong and Fei,  Hao and Qu,  Leigang and Ji,  Wei and Chua,  Tat-Seng},
  keywords = {Artificial Intelligence (cs.AI),  Computation and Language (cs.CL),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {NExT-GPT: Any-to-Any Multimodal LLM},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@article{Yin2024,
    author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
    title = {A survey on multimodal large language models},
    journal = {National Science Review},
    volume = {11},
    number = {12},
    pages = {nwae403},
    year = {2024},
    month = {11},
    abstract = {Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.},
    issn = {2095-5138},
    doi = {10.1093/nsr/nwae403},
    url = {https://doi.org/10.1093/nsr/nwae403},
    eprint = {https://academic.oup.com/nsr/article-pdf/11/12/nwae403/61201557/nwae403.pdf},
}

@inproceedings{
Zhang2020:,
title={BERTScore: Evaluating Text Generation with BERT},
author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@MISC {ibgecenso2022,
    author = "IBGE",
    title  = "Censo Demográfico 2022",
    year   = "2023",
    url    = "{https://biblioteca.ibge.gov.br/visualizacao/livros/liv102038.pdf}"
}

@misc{Brasil2008,
  author    = {Ministério da Educação},
  title     = {Política Nacional de Educação Especial na Perspectiva da Educação Inclusiva},
  year      = {2008},
  url       = {http://portal.mec.gov.br/arquivos/pdf/politicaeducespecial.pdf},
}

@article{BORGES2021,
  title = {Recursos de Acessibilidade e o Uso dos Dispositivos Móveis como Tecnologia Assistiva por Pessoas com Baixa Visão},
  volume = {27},
  ISSN = {1413-6538},
  url = {http://dx.doi.org/10.1590/1980-54702021v27e0036},
  DOI = {10.1590/1980-54702021v27e0036},
  journal = {Revista Brasileira de Educa\c{c}ão Especial},
  publisher = {FapUNIFESP (SciELO)},
  author = {BORGES,  Wanessa Ferreira and MENDES,  Eniceia Gon\c{c}alves},
  year = {2021}
}